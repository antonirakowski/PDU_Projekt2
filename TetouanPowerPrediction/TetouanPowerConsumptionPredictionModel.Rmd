---
title: "Tetouan Power Consumption Prediction"
output: pdf_document
---

In this notebook we will evaluate the influence of weather on power 
consumption/power diffusion trends. Then we will train adequate ML models

Loading necessary data and libraries

```{r}
library(dplyr)
library(ggplot2)
library(png)
library(grid)
weather <- read.csv("TetouanWeather2017.csv", skip = 3)
power <- read.csv("TetouanPowerConsumption2017.csv")
```

Let's inspect the power data for missing values and outliers

```{r}
head(power)
```

```{r}
summary(power)
```

No missing values

Let's inspect the weather data for missing values and outliers

```{r}
head(weather)
```

```{r}
summary(weather)
```

No missing values

Adjusting the date format to enable inner_joing, then inner_joining

```{r}
power %>% mutate(Datetime = format(strptime(Datetime, "%m/%d/%Y %H:%M"), "%Y-%m-%dT%H:00")) -> power_date_converted
joined_data <- inner_join(power_date_converted, weather, by = c("Datetime" = "time"))
```

```{r}
head(joined_data)
```

Let's study the correlations between power usage/diffusion and weather conditions 

```{r}
names(joined_data)
```

```{r}
check_correlation <- function(data, i1, i2, i3, i4) {
  for (x in names(data)[i1:i2]) {
    for (y in names(data)[i3:i4]) {
      correlation <- cor(data[[x]], data[[y]], method = "spearman")
      if (is.na(correlation)) next
      if (abs(correlation) >= 0.80) {   
        cat(sprintf("Correlation of %f between %s and %s.\n", correlation, x, y))
        next
      }
      if (abs(correlation) >= 0.70) {
        cat(sprintf("Correlation of %f between %s and %s.\n", correlation, x, y))
        next
      }
      if (abs(correlation) >= 0.60) {
        cat(sprintf("Correlation of %f between %s and %s.\n", correlation, x, y))
        next
      }
    }
  }
}
```

```{r}
check_correlation(joined_data, 2, 6, 7, 26)
```

Conclusion 1: General diffuse flows seem to be influenced by the evapotranspiration rate and vapour pressure deficits

Conclusion 2: Diffuse flows seem to be influenced by the evapotranspiration rate

Therefore we will focus on the above mentioned correlations

```{r}
joined_data %>% select(GDF = GeneralDiffuseFlows, DF = DiffuseFlows, ET = et0_fao_evapotranspiration..mm., VPD = vapour_pressure_deficit..kPa.) -> correlated_data
```

```{r}
head(correlated_data)
```

Let's study the influence of outliers on the correlation

```{r}
check_correlation_by_delta <- function(delta, i1, i2, i3, i4) {
  
  lower <- lapply(correlated_data[1:2], function(x) quantile(x, probs = 0.00 + delta / 100))
  upper <- lapply(correlated_data[1:2], function(x) quantile(x, probs = 1.00 - delta / 100))
  data <- correlated_data
  
  for (x in names(lower)) {
    data <- data %>% filter(data[[x]] > lower[[x]] & data[[x]] < upper[[x]])
  }
  
  check_correlation(data, i1, i2, i3, i4)
  
  cat("\n")
}
```


```{r}
for (delta in 0:5) check_correlation_by_delta(delta, 1, 2, 3, 4)
```

The effect of outliers is marginal. Let's normalise the data

```{r}
min_max <- function(data) {
  (data - min(data)) / (max(data) - min(data))
}
```

```{r}
correlated_data %>% mutate(GDF = min_max(GDF), DF = min_max(DF), ET = min_max(ET), VPD = min_max(VPD)) -> normalised_data
```

```{r}
head(normalised_data)
```

Let's plot the normalised data and analyse it

```{r}
plot1 <- ggplot(normalised_data, aes(x = ET, y = GDF)) + geom_point(size = 0.5, color = "blue") + labs(title = "f = GDF(ET)", x ="ET", y = "GDF") + theme_minimal()
print(plot1)
```

```{r}
plot2 <- ggplot(normalised_data, aes(x = VPD, y = GDF)) + geom_point(size = 0.5, color = "blue") + labs(title = "f = GDF(VPD)", x ="VPD", y = "GDF") + theme_minimal()
print(plot2)
```

```{r}
plot3 <- ggplot(normalised_data, aes(x = ET, y = DF)) + geom_point(size = 0.5, color = "blue") + labs(title = "f = DF(ET)", x ="ET", y = "DF") + theme_minimal()
print(plot3)
```

The data is clustered, therefore we will try the following: random forest regressor, gradient boosting, k-nearest neighbours regressor. First we must save the data to a csv file and switch to Python

```{r}
normalised_data %>% mutate(GDF = format(GDF, scientific = FALSE), DF = format(DF, scientific = FALSE), ET = format(ET, scientific = FALSE), VPD = format(VPD, scientific = FALSE)) -> data_to_save
```

```{r}
head(data_to_save)
```

```{r}
write.csv(data_to_save, "NormalisedData.csv", row.names = FALSE)
```

Switching to Python and importing necessary libraries

```{python}
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split 
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd 
import matplotlib.pyplot as plt
```

Loading the data into a Panda's dataframe

```{python}
data = pd.read_csv("NormalisedData.csv", header = 0)
data.head()
```

Creating three sets of of features and targets:

1. Features based on ET and VPD to predict target GDF

2. Features based on ET to predict target GDF

3. Features based on ET to predict target DF

```{python}
TargetGDF = data[["GDF"]].values.ravel()
TargetGDF
```

```{python}
FeatureGDF1 = data[["ET"]]
FeatureGDF1.head()
```

```{python}
FeatureGDF2 = data[["ET", "VPD"]]
FeatureGDF2.head()
```

```{python}
TargetDF = data[["DF"]].values.ravel()
TargetDF
```

```{python}
FeatureDF = FeatureGDF1
FeatureDF.head()
```

Creating training and testing data from the above features and targets

```{python}
GDF_Xtrain1, GDF_Xtest1, GDF_Ytrain1, GDF_Ytest1 = train_test_split(FeatureGDF1, TargetGDF, test_size = 0.2, random_state = 666)

GDF_Xtrain2, GDF_Xtest2, GDF_Ytrain2, GDF_Ytest2 = train_test_split(FeatureGDF2, TargetGDF, test_size = 0.2, random_state = 666)

DF_Xtrain, DF_Xtest, DF_Ytrain, DF_Ytest = train_test_split(FeatureDF, TargetDF, test_size = 0.2, random_state = 666)
```

First, let's train models that predict GDF

```{python}

# KNeighborsRegressor trained on ET

knn_regressor1 = KNeighborsRegressor(n_neighbors = 5)
knn_regressor1.fit(GDF_Xtrain1, GDF_Ytrain1)
knn_predictions1 = knn_regressor1.predict(GDF_Xtest1)

# KNeighborsRegressor trained on ET, VPD

knn_regressor2 = KNeighborsRegressor(n_neighbors = 5)
knn_regressor2.fit(GDF_Xtrain2, GDF_Ytrain2)
knn_predictions2 = knn_regressor2.predict(GDF_Xtest2)

# GradientBoostingRegressor trained on ET

gb_regressor1 = GradientBoostingRegressor(n_estimators = 100, random_state = 666)
gb_regressor1.fit(GDF_Xtrain1, GDF_Ytrain1)
gb_predictions1 = gb_regressor1.predict(GDF_Xtest1)

# GradientBoostingRegressor trained on ET, VPD

gb_regressor2 = GradientBoostingRegressor(n_estimators = 100, random_state = 666)
gb_regressor2.fit(GDF_Xtrain2, GDF_Ytrain2)
gb_predictions2 = gb_regressor2.predict(GDF_Xtest2)

# RandomForestRegressor trained on ET

rf_regressor1 = RandomForestRegressor(n_estimators = 100, random_state = 666)
rf_regressor1.fit(GDF_Xtrain1, GDF_Ytrain1)
rf_predictions1 = rf_regressor1.predict(GDF_Xtest1)

# RandomForestRegressor trained on ET, VPD

rf_regressor2 = RandomForestRegressor(n_estimators = 100, random_state = 666)
rf_regressor2.fit(GDF_Xtrain2, GDF_Ytrain2)
rf_predictions2 = rf_regressor2.predict(GDF_Xtest2)
```

Now we shall evaluate the models

```{python}
knn1_mse = mean_squared_error(GDF_Ytest1, knn_predictions1)
knn1_r2 = r2_score(GDF_Ytest1, knn_predictions1)
print(f'KNeighborsRegressor1 MSE: {knn1_mse}')
print(f'KNeighborsRegressor1 R2: {knn1_r2}')
print()

knn2_mse = mean_squared_error(GDF_Ytest2, knn_predictions2)
knn2_r2 = r2_score(GDF_Ytest2, knn_predictions2)
print(f'KNeighborsRegressor2 MSE: {knn2_mse}')
print(f'KNeighborsRegressor2 R2: {knn2_r2}')
print()

gb1_mse = mean_squared_error(GDF_Ytest1, gb_predictions1)
gb1_r2 = r2_score(GDF_Ytest1, gb_predictions1)
print(f'GradientBoostingRegressor1 MSE: {gb1_mse}')
print(f'GradientBoostingRegressor1 R2: {gb1_r2}')
print()

gb2_mse = mean_squared_error(GDF_Ytest2, gb_predictions2)
gb2_r2 = r2_score(GDF_Ytest2, gb_predictions2)
print(f'GradientBoostingRegressor2 MSE: {gb2_mse}')
print(f'GradientBoostingRegressor2 R2: {gb2_r2}')
print()

rf1_mse = mean_squared_error(GDF_Ytest1, rf_predictions1)
rf1_r2 = r2_score(GDF_Ytest1, rf_predictions1)
print(f'RandomForestRegressor1 MSE: {rf1_mse}')
print(f'RandomForestRegressor1 R2: {rf1_r2}')
print()

rf2_mse = mean_squared_error(GDF_Ytest2, rf_predictions2)
rf2_r2 = r2_score(GDF_Ytest2, rf_predictions2)
print(f'RandomForestRegressor2 MSE: {rf2_mse}')
print(f'RandomForestRegressor2 R2: {rf2_r2}')
```

The rf_regressor2 proved to be most accurate. Let's train it again with more n_estimators

```{python}
final_model = RandomForestRegressor(n_estimators = 1000, random_state = 666)
final_model.fit(GDF_Xtrain2, GDF_Ytrain2)
final_predictions = final_model.predict(GDF_Xtest2)
```

Now let's create a scatter plot to demonstrate the results

```{python}
def plot_predictions(y_true, y_pred, title, fname):
  plt.figure(figsize=(8, 8))
  plt.scatter(y_true, y_pred, alpha = 0.5)
  plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], "r--", lw = 2)
  plt.xlabel("True (normalised) values")
  plt.ylabel("Predicted (normalised) values")
  plt.title(title)
  plt.savefig(fname)
```

```{python}
plot_predictions(GDF_Ytest2, final_predictions, "GDF predictions accuracy", "GDFPredictions")
```

```{r}
img <- readPNG("GDFPredictions.png")
grid.newpage()
grid.raster(img)
```
Now let's train the models to predict DF

```{python}
knn_regressor = KNeighborsRegressor(n_neighbors = 5)
knn_regressor.fit(DF_Xtrain, DF_Ytrain)
knn_predictions = knn_regressor.predict(DF_Xtest)

gb_regressor = GradientBoostingRegressor(n_estimators = 100, random_state = 666)
gb_regressor.fit(DF_Xtrain, DF_Ytrain)
gb_predictions = gb_regressor.predict(DF_Xtest)

rf_regressor = RandomForestRegressor(n_estimators = 100, random_state = 666)
rf_regressor.fit(DF_Xtrain, DF_Ytrain)
rf_predictions = rf_regressor.predict(DF_Xtest)
```

Evaluating the models

```{python}
knn_mse = mean_squared_error(DF_Ytest, knn_predictions)
knn_r2 = r2_score(DF_Ytest, knn_predictions)
print(f'KNeighborsRegressor MSE: {knn_mse}')
print(f'KNeighborsRegressor R2: {knn_r2}')
print()

gb_mse = mean_squared_error(DF_Ytest, gb_predictions)
gb_r2 = r2_score(DF_Ytest, gb_predictions)
print(f'GradientBoostingRegressor MSE: {gb_mse}')
print(f'GradientBoostingRegressor R2: {gb_r2}')
print()

rf_mse = mean_squared_error(DF_Ytest, rf_predictions)
rf_r2 = r2_score(DF_Ytest, rf_predictions)
print(f'RandomForestRegressor MSE: {rf_mse}')
print(f'RandomForestRegressor R2: {rf_r2}')
```

The models are insufficiently accurate for use